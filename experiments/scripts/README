[First]
132 x 132 png files, WITHOUT any standalone '1_x' images
==========================
png pixel size 132
train_size 3730
test_size 3169
anchor_size [8, 16, 32]
feature stride size [16,]
logfile slurm-2153455.out, slurm-2155783.out, slurm-2156848.out

[Second]
132 x 132 png files, plus additional 3845 standalone '1_x' images,
all of which are added to the training set (trainsecond.txt). Thus,
trainsecond.txt = train.txt + additional_3845
The testing set remains the same (test.txt)
==========================
png pixel size 132
train_size 3730 + 3845 = 7575
test_size 3169
anchor_size [8, 16, 32]
feature stride size [16,]
logfile slurm-2160201.out, slurm-2163383.out

[Third]
132 x 132 png files, plus additional 3845 '1_x' images, 2411 of which are added
to the training set (trainthird.txt), and 1434 of which are added to the
testing set (testthird.txt).
==========================
png pixel size 132
train_size 3730 + 2411 = 6141
test_size 3169 + 1434 = 4603
anchor_size [8, 16, 32]
feature stride size [16,]
logfile slurm-2163554.out, slurm-2164344.out

[Thirdsmall]
The same as [Third] except that using a smaller anchor scale and a smaller
feature stride size. Training set becomes (trainthirdsmall.txt), which is just
a symbolic link pointing to trainthird.txt. The reason is only to make the name
different to the system. Testing set (testthirdsmall.txt) is also a symbolic
link to testthird.txt
============================
png pixel size 132
train_size 3730 + 2411 = 6141
test_size 3169 + 1434 = 4603
anchor_size [2, 4, 8]
feature stride size [4,]
logfile slurm-2164121.out, slurm-2165140.out
no result

[Fourth]
500 x 500 png files, plus additional 3845 '1_x' images, 2411 of which are added
to the training set (trainfourth.txt), and 1434 of which are added to the
testing set (trainfourth.txt).
============================
png pixel size 500
train_size 3730 + 2411 = 6141
test_size 3169 + 1434 = 4603
anchor_size [8, 16, 32]
feature stride size [16,]

AP for 1_1 = 0.5265
AP for 1_2 = 0.1275
AP for 1_3 = 0.1919
AP for 2_2 = 0.4597
AP for 2_3 = 0.4076
AP for 3_3 = 0.5094
Mean AP = 0.3704
~~~~~~~~
Results:
0.527
0.127
0.192
0.460
0.408
0.509
0.370
~~~~~~~~

[Fifth]
132 png files
components-based bbox,
change config to 132 x 132
70000 iterations
No result
anchor_size [2, 4, 8]
feature stride size [16,]

[Sixth]
132 png files
anchor_size [2, 4, 8]
feature stride size [16,]
ra-dec bbox,
change config to 132 x 132
140000 iterations
No result

[07]
132 png file
anchor_size [8, 16, 32]
feature stride size [16,]
used the ra-dec bbox with adjustment
trainable = True for low-level ConvNets
but change the side back to s=600
currently in the third round of training after (110K iterations, 24 hours)

test07_2nd (110k iterations)
AP for 1_1 = 0.3915
AP for 1_2 = 0.4812
AP for 1_3 = 0.7695
AP for 2_2 = 0.4841
AP for 2_3 = 0.4449
AP for 3_3 = 0.6207
Mean AP = 0.5320

test07_3rd (160k iterations)
AP for 1_1 = 0.3914
AP for 1_2 = 0.4932
AP for 1_3 = 0.7830
AP for 2_2 = 0.4880
AP for 2_3 = 0.4913
AP for 3_3 = 0.4794
Mean AP = 0.5210

[08]
change side to s=132 again
anchor_size [2, 4, 8]
feature stride size [16,]
It still uses z-scale

[09]
s=600
THe same as 07 except it uses logminmax scaled png directly from FITS
After 55K iterations
AP for 1_1 = 0.3769
AP for 1_2 = 0.5523
AP for 1_3 = 0.7012
AP for 2_2 = 0.3207
AP for 2_3 = 0.4410
AP for 3_3 = 0.6884
Mean AP = 0.5134

[10]
change to the new mean based on log_minmax scale  (235, 128, 24)
similar to 09, it uses log_minmax scaled png
similar to 08, it uses s=132
Does not work!

[11]
The same as 10, BUT with s=600
AP for 1_1 = 0.3701
AP for 1_2 = 0.4665
AP for 1_3 = 0.6128
AP for 2_2 = 0.2377
AP for 2_3 = 0.4292
AP for 3_3 = 0.6339
Mean AP = 0.4584

[12]
The same as 11, BUT
anchor_scales=[1, 2, 4, 8, 16, 32]
anchor_ratios=[1]
RPN_BATCHSIZE is still 256
55K iterations:
-------------------
AP for 1_1 = 0.5186
AP for 1_2 = 0.5417
AP for 1_3 = 0.6363
AP for 2_2 = 0.3562
AP for 2_3 = 0.4369
AP for 3_3 = 0.5244
Mean AP = 0.5023

105K Iterations (with a starting LR 0.001 for the second round)
-------------------
AP for 1_1 = 0.2887
AP for 1_2 = 0.6552
AP for 1_3 = 0.7526
AP for 2_2 = 0.5021
AP for 2_3 = 0.5364
AP for 3_3 = 0.7506
Mean AP = 0.5809

100K Iterations (with a starting LR 0.0001 for the second round)
???

[13]
The same as [12] but loading the original VGGnet feature ConvNet
low_level_trainable is still TRUE
50K iteration
AP for 1_1 = 0.5088
AP for 1_2 = 0.7385
AP for 1_3 = 0.8281
AP for 2_2 = 0.5242
AP for 2_3 = 0.6716
AP for 3_3 = 0.7821
Mean AP = 0.6755

100K iteration (with a starting LR 0.0001)
AP for 1_1 = 0.4789
AP for 1_2 = 0.7137
AP for 1_3 = 0.8989
AP for 2_2 = 0.7050
AP for 2_3 = 0.7712
AP for 3_3 = 0.9068
Mean AP = 0.7458

[13.1]
do not load pretrained weights for fc6 and fc7 (Skipping)
not running yet

[14]
The same as [13] but loading the original VGGnet feature ConvNet
low_level_trainable is set to FALSE
do not skip fc6 and fc7
70K iteration
AP for 1_1 = 0.4616
AP for 1_2 = 0.7060
AP for 1_3 = 0.8937
AP for 2_2 = 0.6699
AP for 2_3 = 0.7823
AP for 3_3 = 0.9114
Mean AP = 0.7375

[14.1]
reduce RPN_MIN_SIZE to 4
skip fc6 and fc7, after 65K iterations
AP for 1_1 = 0.8088
AP for 1_2 = 0.6969
AP for 1_3 = 0.8922
AP for 2_2 = 0.7113
AP for 2_3 = 0.7184
AP for 3_3 = 0.8847
Mean AP = 0.7854

after 80K Iterations
AP for 1_1 = 0.8068
AP for 1_2 = 0.7099
AP for 1_3 = 0.8972
AP for 2_2 = 0.7165
AP for 2_3 = 0.7041
AP for 3_3 = 0.8878
Mean AP = 0.7870

after 80K iterations, TEST.RPN_POST_NMS_TOP_N = 30
AP for 1_1 = 0.8060
AP for 1_2 = 0.7116
AP for 1_3 = 0.8886
AP for 2_2 = 0.7243
AP for 2_3 = 0.7165
AP for 3_3 = 0.8921
Mean AP = 0.7899

after 80K iterations, TEST.RPN_POST_NMS_TOP_N = 5
AP for 1_1 = 0.8185
AP for 1_2 = 0.7179
AP for 1_3 = 0.8901
AP for 2_2 = 0.7439
AP for 2_3 = 0.7162
AP for 3_3 = 0.8881
Mean AP = 0.7958
/home/cwu/rgz-ml-ws/code/output/faster_rcnn_end2end/rgz_2017_train14_gold/VGGnet_fast_rcnn-15000


after 160K iterations, in the last 80K, freeze other parameters,
but only train box class/regressor,
and freezes all other weights (including Cvnet)

AP for 1_1 = 0.8297
AP for 1_2 = 0.7102
AP for 1_3 = 0.8880
AP for 2_2 = 0.7467
AP for 2_3 = 0.7357
AP for 3_3 = 0.8902
Mean AP = 0.8001
/group/pawsey0129/cwu/rgz-faster-rcnn/output/faster_rcnn_end2end/rgz_2017_train14/VGGnet_fast_rcnn-95000


after 85K iterations, TEST.RPN_POST_NMS_TOP_N = 5
AP for 1_1 = 0.8081
AP for 1_2 = 0.6836
AP for 1_3 = 0.8928
AP for 2_2 = 0.7279
AP for 2_3 = 0.7473
AP for 3_3 = 0.8816
Mean AP = 0.7902

after 105K, TEST.RPN_POST_NMS_TOP_N = 30
AP for 1_1 = 0.7951
AP for 1_2 = 0.6762
AP for 1_3 = 0.8907
AP for 2_2 = 0.7084
AP for 2_3 = 0.7395
AP for 3_3 = 0.8844
Mean AP = 0.7824

130K iterations, TEST.RPN_POST_NMS_TOP_N = 5
AP for 1_1 = 0.8245
AP for 1_2 = 0.6901
AP for 1_3 = 0.8837
AP for 2_2 = 0.7461
AP for 2_3 = 0.7349
AP for 3_3 = 0.8794
Mean AP = 0.7931
Model is located at:
/home/cwu/rgz-ml-ws/code/output/faster_rcnn_end2end/rgz_2017_train14_gold/VGGnet_fast_rcnn-65000


[15]
The same as [14] but do not load pretrained weights for fc6 and fc7 (Skipping)
70k iterations
AP for 1_1 = 0.5043
AP for 1_2 = 0.7019
AP for 1_3 = 0.8868
AP for 2_2 = 0.6430
AP for 2_3 = 0.7432
AP for 3_3 = 0.8831
Mean AP = 0.7270

140 iterations, LR starting at 0.0001/2 = 0.00005
AP for 1_1 = 0.4405
AP for 1_2 = 0.7017
AP for 1_3 = 0.8935
AP for 2_2 = 0.7011
AP for 2_3 = 0.7337
AP for 3_3 = 0.8968
Mean AP = 0.7279

[16]
The same as 14.1 but with a new channel from IR images!
The data is produced by  prepare_data.mix_radio_ir_train16_old()
After 140k iterations
AP for 1_1 = 0.8123
AP for 1_2 = 0.6847
AP for 1_3 = 0.8542
AP for 2_2 = 0.7087
AP for 2_3 = 0.6995
AP for 3_3 = 0.8455
Mean AP = 0.7675

now redo it with convexHull by mix_radio_ir_train16()
80k iterations
AP for 1_1 = 0.8542
AP for 1_2 = 0.6833
AP for 1_3 = 0.8706
AP for 2_2 = 0.7454
AP for 2_3 = 0.6700
AP for 3_3 = 0.8578
Mean AP = 0.7802

now continue train it, but only train box class/regressor,
and freezes all otherwise weights (including Cvnet)
starting from 80001 iterations til 160000 iterations
AP for 1_1 = 0.8666
AP for 1_2 = 0.6818
AP for 1_3 = 0.8585
AP for 2_2 = 0.7417
AP for 2_3 = 0.6562
AP for 3_3 = 0.8502
Mean AP = 0.7758


[17]
The same as [16], but with IR inserted as:
The data is produced by  prepare_data.mix_radio_ir()
0 Blue = radio [0], 1 Green = radio [1], 2 Red = IR[1]
After 130K iterations
AP for 1_1 = 0.8122
AP for 1_2 = 0.6846
AP for 1_3 = 0.8495
AP for 2_2 = 0.7121
AP for 2_3 = 0.7100
AP for 3_3 = 0.8522
Mean AP = 0.7701


[18]
The same as [16], but with IR inserted as:
The data is produced by  prepare_data.mix_radio_ir()
0 Blue = radio[0], 1 Green = radio [1], 2 Red = IR[1] + Radio [2]
AP for 1_1 = 0.8242
AP for 1_2 = 0.6843
AP for 1_3 = 0.8561
AP for 2_2 = 0.7231
AP for 2_3 = 0.6989
AP for 3_3 = 0.8561
Mean AP = 0.7738

[19]
Use a new kind of IR mix
t[:, :, 0] = im_radio[:, :, 0]  # BLUE
t[:, :, 1] = im_radio[:, :, 1]  # GREEN
t[:, :, 2] = mask_ir[0:w, 0:h, 1]  # cut out from IR image as RED

After 65K iterations
AP for 1_1 = 0.8613
AP for 1_2 = 0.6788
AP for 1_3 = 0.8601
AP for 2_2 = 0.7389
AP for 2_3 = 0.6799
AP for 3_3 = 0.8242
Mean AP = 0.7739

Now running iterations at 125K iterations
/group/pawsey0129/cwu/rgz-faster-rcnn/output/faster_rcnn_end2end/rgz_2017_train19/VGGnet_fast_rcnn-125000
AP for 1_1 = 0.8680
AP for 1_2 = 0.6857
AP for 1_3 = 0.8557
AP for 2_2 = 0.7250
AP for 2_3 = 0.6578
AP for 3_3 = 0.8172
Mean AP = 0.7682

[08]
the same as 14.1 but this time uses z-scale again
After 135,000 iterations (with 0.01, 0.003 and 0.0003 as learning rates)
AP for 1_1 = 0.8087
AP for 1_2 = 0.6376
AP for 1_3 = 0.8250
AP for 2_2 = 0.7474
AP for 2_3 = 0.7708
AP for 3_3 = 0.9230
Mean AP = 0.7854

[20]
The same as 14.1, but adding another 1901 training examples (1_2, and 2_2)
After 135K iterations
AP for 1_1 = 0.8016
AP for 1_2 = 0.7298
AP for 1_3 = 0.8677
AP for 2_2 = 0.7404
AP for 2_3 = 0.7323
AP for 3_3 = 0.8859
Mean AP = 0.7929

After 270K iteratios
AP for 1_1 = 0.8213
AP for 1_2 = 0.7023
AP for 1_3 = 0.8484
AP for 2_2 = 0.7373
AP for 2_3 = 0.7068
AP for 3_3 = 0.8584
Mean AP = 0.7791

[21]
Using the IR image with contours (sigma = 5)
TEST.RPN_POST_NMS_TOP_N = 5
55K Iterations
AP for 1_1 = 0.8176
AP for 1_2 = 0.6931
AP for 1_3 = 0.8750
AP for 2_2 = 0.7707
AP for 2_3 = 0.8043
AP for 3_3 = 0.9409
Mean AP = 0.8169
/group/pawsey0129/cwu/rgz-faster-rcnn/output/faster_rcnn_end2end/rgz_2017_train21/VGGnet_fast_rcnn-55000

60K Iterations
AP for 1_1 = 0.8467
AP for 1_2 = 0.6729
AP for 1_3 = 0.8773
AP for 2_2 = 0.7743
AP for 2_3 = 0.8168
AP for 3_3 = 0.9469
Mean AP = 0.8225
model location:
/group/pawsey0129/cwu/rgz-faster-rcnn/output/faster_rcnn_end2end/rgz_2017_train21/VGGnet_fast_rcnn-60000

65K Iterations
AP for 1_1 = 0.8386
AP for 1_2 = 0.6797
AP for 1_3 = 0.8774
AP for 2_2 = 0.7742
AP for 2_3 = 0.8082
AP for 3_3 = 0.9414
Mean AP = 0.8199
model location:
/group/pawsey0129/cwu/rgz-faster-rcnn/output/faster_rcnn_end2end/rgz_2017_train21/VGGnet_fast_rcnn-65000

70K iterations
AP for 1_1 = 0.8420
AP for 1_2 = 0.6771
AP for 1_3 = 0.8724
AP for 2_2 = 0.7742
AP for 2_3 = 0.8010
AP for 3_3 = 0.9472
Mean AP = 0.8190
model location:
/group/pawsey0129/cwu/rgz-faster-rcnn/output/faster_rcnn_end2end/rgz_2017_train21/VGGnet_fast_rcnn-70000

But 135K iterations, got worse
AP for 1_1 = 0.8445
AP for 1_2 = 0.6410
AP for 1_3 = 0.8636
AP for 2_2 = 0.7735
AP for 2_3 = 0.8008
AP for 3_3 = 0.9396
Mean AP = 0.8105

[22]
Using the IR image with contours, also apply mask, but replace anything outside
mask with mean value of the background

60K Iterations
AP for 1_1 = 0.8532
AP for 1_2 = 0.6749
AP for 1_3 = 0.8851
AP for 2_2 = 0.7954
AP for 2_3 = 0.7976
AP for 3_3 = 0.9385
Mean AP = 0.8241

Currently (26 Aug 2017) running [22] again (jobid - 2212360)
but with new st_pool (bug fixed)

100K Iterations
AP for 1_1 = 0.8773
AP for 1_2 = 0.6988
AP for 1_3 = 0.8618
AP for 2_2 = 0.8099
AP for 2_3 = 0.7610
AP for 3_3 = 0.9253
Mean AP = 0.8224

80K Iterations (Gold model stored)
AP for 1_1 = 0.8854
AP for 1_2 = 0.6957
AP for 1_3 = 0.8718
AP for 2_2 = 0.8131
AP for 2_3 = 0.7732
AP for 3_3 = 0.9320
Mean AP = 0.8285

70K Iterations
AP for 1_1 = 0.8749
AP for 1_2 = 0.6721
AP for 1_3 = 0.8751
AP for 2_2 = 0.8036
AP for 2_3 = 0.7769
AP for 3_3 = 0.9281
Mean AP = 0.8218

65K Iterations
AP for 1_1 = 0.8719
AP for 1_2 = 0.6852
AP for 1_3 = 0.8822
AP for 2_2 = 0.8143
AP for 2_3 = 0.7751
AP for 3_3 = 0.9291
Mean AP = 0.8263

60K Iterations
AP for 1_1 = 0.8836
AP for 1_2 = 0.6797
AP for 1_3 = 0.8746
AP for 2_2 = 0.8209
AP for 2_3 = 0.7788
AP for 3_3 = 0.9230
Mean AP = 0.8268

90k Iterations
AP for 1_1 = 0.8699
AP for 1_2 = 0.6097
AP for 1_3 = 0.8781
AP for 2_2 = 0.7937
AP for 2_3 = 0.7616
AP for 3_3 = 0.9182
Mean AP = 0.8052

160K iterations
AP for 1_1 = 0.8774
AP for 1_2 = 0.6828
AP for 1_3 = 0.8578
AP for 2_2 = 0.7967
AP for 2_3 = 0.7574
AP for 3_3 = 0.9254
Mean AP = 0.8162

Currently set cfg.TRAIN.FG_THRESH to 0.7
50K iterations
AP for 1_1 = 0.8309
AP for 1_2 = 0.5119
AP for 1_3 = 0.8320
AP for 2_2 = 0.6698
AP for 2_3 = 0.6686
AP for 3_3 = 0.8452
Mean AP = 0.7264

60K iterations
AP for 1_1 = 0.8581
AP for 1_2 = 0.5566
AP for 1_3 = 0.8575
AP for 2_2 = 0.7172
AP for 2_3 = 0.6770
AP for 3_3 = 0.8744
Mean AP = 0.7568

So it appears there is an over-fitting problem

[with "priority negative mode"]
60K iterations
AP for 1_1 = 0.8674
AP for 1_2 = 0.6601
AP for 1_3 = 0.8782
AP for 2_2 = 0.7936
AP for 2_3 = 0.7740
AP for 3_3 = 0.9264
Mean AP = 0.8166


80K iterations
AP for 1_1 = 0.8801
AP for 1_2 = 0.6500
AP for 1_3 = 0.8734
AP for 2_2 = 0.8017
AP for 2_3 = 0.7554
AP for 3_3 = 0.9243
Mean AP = 0.8142

[22.1]
The same as [22], but do not use the actual pixel mean, but use the default
VGGNet ImageNet mean: 103.939,  116.779,  123.68

80K iterations
AP for 1_1 = 0.8902
AP for 1_2 = 0.6588
AP for 1_3 = 0.8906
AP for 2_2 = 0.8216
AP for 2_3 = 0.7879
AP for 3_3 = 0.9321
Mean AP = 0.8302

[22.2]
Continue from [22.1] but freeze all weights in ConvNet, and set initial lr to
0.0001
80K iterations
AP for 1_1 = 0.8857
AP for 1_2 = 0.6779
AP for 1_3 = 0.8792
AP for 2_2 = 0.8318
AP for 2_3 = 0.7769
AP for 3_3 = 0.9273
Mean AP = 0.8298

[22.3]
The same as 22.2 except that lr still starts from 0.001
80K iterations
AP for 1_1 = 0.8848
AP for 1_2 = 0.6907
AP for 1_3 = 0.8697
AP for 2_2 = 0.8252
AP for 2_3 = 0.7747
AP for 3_3 = 0.9291
Mean AP = 0.8290

[14]
Currently running [14] again (jobid - 2212370) but differences:
1. using st_pool (bug fixed)
2. only load low-level kernels, and freeze them during training.
   The reason only load low-level kernels is because if we also the high
   level kernels at Conv3_1 ~ Conv5_3, the numerical stability becomes so bad
   that it overflows the exp(wd) operations in the first 30 iterations. e.g.
   ===
   RuntimeWarning: overflow encountered in exp
   RuntimeWarning: overflow encountered in multiply
   W tensorflow/core/framework/op_kernel.cc:993] Invalid argument: Incompatible shapes: [128,28] vs. [37,28]
   ===
   The reason for that is because since the st_pool propagates back the gradient
   of the regional proposals' coordinates, that apparently did not work with the
   ImageNet pre-trained weights values.

   Results after 120K iterations
   0.849
   0.675
   0.904
   0.735
   0.719
   0.876
   0.793

   But the mean values of the images are all wrong, so now running with the correct
   mean, see what happens, but with priority_negative = True
